{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with some California Census Data, we'll be trying to use various features of an individual to predict what class of income they belogn in (>50k or <=50k). \n",
    "\n",
    "Here is some information about the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Column Name</th>\n",
    "<th>Type</th>\n",
    "<th>Description</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>age</td>\n",
    "<td>Continuous</td>\n",
    "<td>The age of the individual</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>workclass</td>\n",
    "<td>Categorical</td>\n",
    "<td>The type of employer the  individual has (government,  military, private, etc.).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>fnlwgt</td>\n",
    "<td>Continuous</td>\n",
    "<td>The number of people the census  takers believe that observation  represents (sample weight). This  variable will not be used.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>education</td>\n",
    "<td>Categorical</td>\n",
    "<td>The highest level of education  achieved for that individual.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>education_num</td>\n",
    "<td>Continuous</td>\n",
    "<td>The highest level of education in  numerical form.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>marital_status</td>\n",
    "<td>Categorical</td>\n",
    "<td>Marital status of the individual.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>occupation</td>\n",
    "<td>Categorical</td>\n",
    "<td>The occupation of the individual.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>relationship</td>\n",
    "<td>Categorical</td>\n",
    "<td>Wife, Own-child, Husband,  Not-in-family, Other-relative,  Unmarried.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>race</td>\n",
    "<td>Categorical</td>\n",
    "<td>White, Asian-Pac-Islander,  Amer-Indian-Eskimo, Other, Black.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>gender</td>\n",
    "<td>Categorical</td>\n",
    "<td>Female, Male.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>capital_gain</td>\n",
    "<td>Continuous</td>\n",
    "<td>Capital gains recorded.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>capital_loss</td>\n",
    "<td>Continuous</td>\n",
    "<td>Capital Losses recorded.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>hours_per_week</td>\n",
    "<td>Continuous</td>\n",
    "<td>Hours worked per week.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>native_country</td>\n",
    "<td>Categorical</td>\n",
    "<td>Country of origin of the  individual.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>income</td>\n",
    "<td>Categorical</td>\n",
    "<td>\"&gt;50K\" or \"&lt;=50K\", meaning  whether the person makes more  than \\$50,000 annually.</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow the Directions in Bold. If you get stuck, check out the solutions lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Read in the census_data.csv data with pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: No module named 'pandas._libs.tslib' not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace --force' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     from pandas._libs import (hashtable as _hashtable,\n\u001b[0m\u001b[0;32m     27\u001b[0m                              \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtslib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimedelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas._libs.tslib'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-af55e7023913>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                       \u001b[1;34m\"pandas from the source directory, you may need to run \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                       \u001b[1;34m\"'python setup.py build_ext --inplace --force' to build \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                       \"the C extensions first.\".format(module))\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: C extension: No module named 'pandas._libs.tslib' not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace --force' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "census = pd.read_csv('census_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'census' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-027d74015e5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcensus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'census' is not defined"
     ]
    }
   ],
   "source": [
    "census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TensorFlow won't be able to understand strings as labels, you'll need to use pandas .apply() method to apply a custom function that converts them to 0s and 1s. This might be hard if you aren't very familiar with pandas, so feel free to take a peek at the solutions for this part.**\n",
    "\n",
    "** Convert the Label column to 0s and 1s instead of strings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a Train Test Split on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Feature Columns for tf.esitmator\n",
    "\n",
    "** Take note of categorical vs continuous values! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'education', 'education_num', 'marital_status',\n",
       "       'occupation', 'relationship', 'race', 'gender', 'capital_gain',\n",
       "       'capital_loss', 'hours_per_week', 'native_country', 'income_bracket'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import Tensorflow **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Create the tf.feature_columns for the categorical values. Use vocabulary lists or just use hash buckets. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create the continuous feature_columns for the continuous values using numeric_column **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Put all these variables into a single list with the variable name feat_cols **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Function\n",
    "\n",
    "** Batch_size is up to you. But do make sure to shuffle!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create your model with tf.estimator\n",
    "\n",
    "**Create a LinearClassifier.(If you want to use a DNNClassifier, keep in mind you'll need to create embedded columns out of the cateogrical feature that use strings, check out the previous lecture on this for more info.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Marcial\\AppData\\Local\\Temp\\tmpyyyqnxgv\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_save_checkpoints_secs': 600, '_model_dir': 'C:\\\\Users\\\\Marcial\\\\AppData\\\\Local\\\\Temp\\\\tmpyyyqnxgv', '_keep_checkpoint_max': 5, '_log_step_count_steps': 100, '_save_summary_steps': 100, '_tf_random_seed': 1}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train your model on the data, for at least 5000 steps. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Marcial\\AppData\\Local\\Temp\\tmpyyyqnxgv\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 69.3147\n",
      "INFO:tensorflow:global_step/sec: 96.9254\n",
      "INFO:tensorflow:step = 101, loss = 111.492 (1.033 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.482\n",
      "INFO:tensorflow:step = 201, loss = 86.7846 (0.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.543\n",
      "INFO:tensorflow:step = 301, loss = 42.916 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.051\n",
      "INFO:tensorflow:step = 401, loss = 395.305 (0.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.836\n",
      "INFO:tensorflow:step = 501, loss = 106.833 (0.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.406\n",
      "INFO:tensorflow:step = 601, loss = 308.781 (0.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.19\n",
      "INFO:tensorflow:step = 701, loss = 151.164 (0.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.606\n",
      "INFO:tensorflow:step = 801, loss = 254.651 (0.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.324\n",
      "INFO:tensorflow:step = 901, loss = 88.3666 (0.959 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.488\n",
      "INFO:tensorflow:step = 1001, loss = 31.7097 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.138\n",
      "INFO:tensorflow:step = 1101, loss = 409.453 (0.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.448\n",
      "INFO:tensorflow:step = 1201, loss = 170.47 (0.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.189\n",
      "INFO:tensorflow:step = 1301, loss = 83.8109 (0.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.084\n",
      "INFO:tensorflow:step = 1401, loss = 42.0138 (0.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.835\n",
      "INFO:tensorflow:step = 1501, loss = 157.388 (0.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.765\n",
      "INFO:tensorflow:step = 1601, loss = 449.153 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.149\n",
      "INFO:tensorflow:step = 1701, loss = 44.765 (0.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.215\n",
      "INFO:tensorflow:step = 1801, loss = 168.323 (0.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.031\n",
      "INFO:tensorflow:step = 1901, loss = 75.7545 (0.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.984\n",
      "INFO:tensorflow:step = 2001, loss = 197.694 (0.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.766\n",
      "INFO:tensorflow:step = 2101, loss = 197.514 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.047\n",
      "INFO:tensorflow:step = 2201, loss = 94.6208 (0.990 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.835\n",
      "INFO:tensorflow:step = 2301, loss = 214.858 (0.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.204\n",
      "INFO:tensorflow:step = 2401, loss = 31.8486 (0.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.134\n",
      "INFO:tensorflow:step = 2501, loss = 99.127 (0.979 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.661\n",
      "INFO:tensorflow:step = 2601, loss = 31.7933 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.293\n",
      "INFO:tensorflow:step = 2701, loss = 86.2802 (0.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.488\n",
      "INFO:tensorflow:step = 2801, loss = 70.7707 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.03\n",
      "INFO:tensorflow:step = 2901, loss = 56.5935 (0.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.434\n",
      "INFO:tensorflow:step = 3001, loss = 224.287 (0.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.997\n",
      "INFO:tensorflow:step = 3101, loss = 54.2692 (0.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.269\n",
      "INFO:tensorflow:step = 3201, loss = 29.8634 (0.959 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.512\n",
      "INFO:tensorflow:step = 3301, loss = 39.41 (0.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.597\n",
      "INFO:tensorflow:step = 3401, loss = 299.747 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.488\n",
      "INFO:tensorflow:step = 3501, loss = 163.921 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.565\n",
      "INFO:tensorflow:step = 3601, loss = 76.9854 (0.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.888\n",
      "INFO:tensorflow:step = 3701, loss = 70.6709 (0.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.345\n",
      "INFO:tensorflow:step = 3801, loss = 50.6713 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.27\n",
      "INFO:tensorflow:step = 3901, loss = 36.4178 (0.959 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.907\n",
      "INFO:tensorflow:step = 4001, loss = 26.5719 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.351\n",
      "INFO:tensorflow:step = 4101, loss = 45.6485 (0.968 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.602\n",
      "INFO:tensorflow:step = 4201, loss = 34.9826 (0.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.727\n",
      "INFO:tensorflow:step = 4301, loss = 403.137 (0.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.765\n",
      "INFO:tensorflow:step = 4401, loss = 49.5894 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.619\n",
      "INFO:tensorflow:step = 4501, loss = 67.7926 (0.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.673\n",
      "INFO:tensorflow:step = 4601, loss = 68.8307 (0.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.977\n",
      "INFO:tensorflow:step = 4701, loss = 49.4603 (0.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.137\n",
      "INFO:tensorflow:step = 4801, loss = 49.6449 (0.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.781\n",
      "INFO:tensorflow:step = 4901, loss = 31.02 (0.963 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\Marcial\\AppData\\Local\\Temp\\tmpyyyqnxgv\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 68.2374.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x259c0be92b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "** Create a prediction input function. Remember to only supprt X_test data and keep shuffle=False. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Use model.predict() and pass in your input function. This will produce a generator of predictions, which you can then transform into a list, with list() **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Marcial\\AppData\\Local\\Temp\\tmpyyyqnxgv\\model.ckpt-5000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Each item in your list will look like this: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_ids': array([0], dtype=int64),\n",
       " 'classes': array([b'0'], dtype=object),\n",
       " 'logistic': array([ 0.21327116], dtype=float32),\n",
       " 'logits': array([-1.30531931], dtype=float32),\n",
       " 'probabilities': array([ 0.78672886,  0.21327116], dtype=float32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a list of only the class_ids key values from the prediction list of dictionaries, these are the predictions you will use to compare against the real y_test values. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import classification_report from sklearn.metrics and then see if you can figure out how to use it to easily get a full report of your model's performance on the test data. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.92      0.90      7436\n",
      "          1       0.70      0.59      0.64      2333\n",
      "\n",
      "avg / total       0.84      0.84      0.84      9769\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
